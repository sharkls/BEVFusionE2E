#!/bin/bash
# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: MIT
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.

# configure the environment
. tool/environment.sh

# æ£€æŸ¥é…ç½®çŠ¶æ€ï¼Œå¦‚æœä¸æ˜¯æˆåŠŸï¼Œåˆ™è¾“å‡ºé”™è¯¯ä¿¡æ¯å¹¶é€€å‡ºã€‚
if [ "$ConfigurationStatus" != "Success" ]; then
    echo "Exit due to configure failure."
    exit
fi

# tensorrt version è·å–TensorRTçš„ç‰ˆæœ¬å·
# version=`trtexec | grep -m 1 TensorRT | sed -n "s/.*\[TensorRT v\([0-9]*\)\].*/\1/p"`

# resnet50/resnet50-int8/swint-tiny # è®¾ç½®æ¨¡å‹çš„åŸºæœ¬è·¯å¾„ï¼Œä½¿ç”¨è°ƒè¯•æ¨¡å‹çš„åç§°ã€‚
base=model/$DEBUG_MODEL

# fp16/int8 è®¾ç½®ç²¾åº¦ï¼Œä½¿ç”¨è°ƒè¯•ç²¾åº¦çš„å€¼ã€‚
precision=$DEBUG_PRECISION

# precision flags æ ¹æ®ç²¾åº¦è®¾ç½®TensorRTæ‰§è¡Œçš„æ ‡å¿—ã€‚å¦‚æœç²¾åº¦æ˜¯int8ï¼Œåˆ™æ·»åŠ ç›¸åº”çš„æ ‡å¿—ã€‚
trtexec_fp16_flags="--fp16"
trtexec_dynamic_flags="--fp16"
if [ "$precision" == "int8" ]; then
    trtexec_dynamic_flags="--fp16 --int8"
fi

# è·å–ONNXæ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºæ•°é‡ã€‚
function get_onnx_number_io(){

    # $1=model
    model=$1

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¾“å‡ºé”™è¯¯ä¿¡æ¯å¹¶è¿”å›ã€‚
    if [ ! -f "$model" ]; then
        echo The model [$model] not exists.
        return
    fi

    # ä½¿ç”¨Pythonè„šæœ¬åŠ è½½ONNXæ¨¡å‹å¹¶è·å–è¾“å…¥å’Œè¾“å‡ºçš„æ•°é‡ã€‚
    number_of_input=`python3 -c "import onnx;m=onnx.load('$model');print(len(m.graph.input), end='')"`
    number_of_output=`python3 -c "import onnx;m=onnx.load('$model');print(len(m.graph.output), end='')"`
    # echo The model [$model] has $number_of_input inputs and $number_of_output outputs.
}

# ç¼–è¯‘TensorRTæ¨¡å‹ï¼Œæ¥æ”¶æ¨¡å‹åç§°ã€ç²¾åº¦æ ‡å¿—ã€è¾“å…¥è¾“å‡ºæ•°é‡å’Œé¢å¤–æ ‡å¿—ã€‚
function compile_trt_model(){

    # $1: name
    # $2: precision_flags
    # $3: number_of_input
    # $4: number_of_output
    # $5: extra_flags
    name=$1
    precision_flags=$2
    number_of_input=$3
    number_of_output=$4
    extra_flags=$5
    result_save_directory=$base/build
    onnx=$base/$name.onnx

    # æ£€æŸ¥æ¨¡å‹çš„è®¡åˆ’æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™è¾“å‡ºä¿¡æ¯å¹¶è¿”å›ã€‚
    if [ -f "${result_save_directory}/$name.plan" ]; then
        echo Model ${result_save_directory}/$name.plan already build ğŸ™‹ğŸ™‹ğŸ™‹.
        return
    fi
    
    # Remove the onnx dependency
    # get_onnx_number_io $onnx
    # echo $number_of_input  $number_of_output

    # æ ¹æ®è¾“å…¥æ•°é‡æ„å»ºè¾“å…¥æ ¼å¼æ ‡å¿—ã€‚
    input_flags="--inputIOFormats="
    output_flags="--outputIOFormats="
    for i in $(seq 1 $number_of_input); do
        input_flags+=fp16:chw,
    done

    # æ ¹æ®è¾“å‡ºæ•°é‡æ„å»ºè¾“å‡ºæ ¼å¼æ ‡å¿—ã€‚
    for i in $(seq 1 $number_of_output); do
        output_flags+=fp16:chw,
    done

    # å»æ‰æœ€åä¸€ä¸ªé€—å·ã€‚
    input_flags=${input_flags%?}
    output_flags=${output_flags%?}

    # æ„å»ºTensorRTç¼–è¯‘å‘½ä»¤ã€‚
    cmd="--onnx=$base/$name.onnx ${precision_flags} ${input_flags} ${output_flags} ${extra_flags} \
        --saveEngine=${result_save_directory}/$name.plan \
        --memPoolSize=workspace:2048 --verbose --dumpLayerInfo \
        --dumpProfile --separateProfileRun \
        --profilingVerbosity=detailed --exportLayerInfo=${result_save_directory}/$name.json"

    # åˆ›å»ºä¿å­˜ç»“æœçš„ç›®å½•ï¼Œå¹¶è¾“å‡ºæ­£åœ¨æ„å»ºæ¨¡å‹çš„ä¿¡æ¯ã€‚
    mkdir -p $result_save_directory
    echo Building the model: ${result_save_directory}/$name.plan, this will take several minutes. Wait a moment ğŸ¤—ğŸ¤—ğŸ¤—~.
    
    # æ‰§è¡Œç¼–è¯‘å‘½ä»¤ï¼Œå¹¶å°†è¾“å‡ºé‡å®šå‘åˆ°æ—¥å¿—æ–‡ä»¶ã€‚å¦‚æœç¼–è¯‘å¤±è´¥ï¼Œåˆ™è¾“å‡ºé”™è¯¯ä¿¡æ¯å¹¶é€€å‡ºã€‚
    trtexec $cmd > ${result_save_directory}/$name.log 2>&1
    if [ $? != 0 ]; then
        echo ğŸ˜¥ Failed to build model ${result_save_directory}/$name.plan.
        echo You can check the error message by ${result_save_directory}/$name.log 
        exit 1
    fi
}

# maybe int8 / fp16     ç¼–è¯‘ä¸¤ä¸ªæ¨¡å‹ï¼Œä½¿ç”¨åŠ¨æ€ç²¾åº¦æ ‡å¿—ã€‚
compile_trt_model "camera.backbone" "$trtexec_dynamic_flags" 2 2
compile_trt_model "fuser" "$trtexec_dynamic_flags" 2 1

# fp16 only ç¼–è¯‘ä¸€ä¸ªä»…ä½¿ç”¨fp16ç²¾åº¦çš„æ¨¡å‹ã€‚
compile_trt_model "camera.vtransform" "$trtexec_fp16_flags" 1 1

# ç¼–è¯‘ä¸€ä¸ªç‰¹å®šçš„æ¨¡å‹ï¼Œå¯èƒ½ä¼šé‡åˆ°TensorRTçš„bugï¼Œä½†é€Ÿåº¦æ›´å¿«ã€‚
# for myelin layernorm head.bbox, may occur a tensorrt bug at layernorm fusion but faster
compile_trt_model "head.bbox" "$trtexec_fp16_flags" 1 6

# ç¼–è¯‘å¦ä¸€ä¸ªç‰ˆæœ¬çš„æ¨¡å‹ï¼Œå‡†ç¡®ä½†é€Ÿåº¦è¾ƒæ…¢ã€‚
# for layernorm version head.bbox.onnx, accurate but slower
# compile_trt_model "head.bbox.layernormplugin" "$trtexec_fp16_flags" 1 6 "--plugins=libcustom_layernorm.so"
